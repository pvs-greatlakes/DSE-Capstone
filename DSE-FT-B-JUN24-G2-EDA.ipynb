{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "e395faf5",
   "metadata": {},
   "source": [
    "# Capstone Project - Interim Report"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c7e2392c",
   "metadata": {},
   "source": [
    "### 1) Importing the required libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "9a2b7bc8",
   "metadata": {},
   "outputs": [],
   "source": [
    "from datetime import datetime\n",
    "from datetime import timedelta\n",
    "import time\n",
    "start_time = time.time()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "c1eeb334",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1.5.1\n"
     ]
    }
   ],
   "source": [
    "# check scikit-learn version\n",
    "import sklearn\n",
    "print(sklearn.__version__)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "43c499cc",
   "metadata": {},
   "outputs": [],
   "source": [
    "import warnings\n",
    "warnings.filterwarnings(\"ignore\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "76b77ad7",
   "metadata": {},
   "outputs": [],
   "source": [
    "import   pandas                 as      pd\n",
    "import   numpy                  as      np\n",
    "import   matplotlib.pyplot      as      plt\n",
    "import   seaborn                as      sns\n",
    "import   scipy.stats            as      stats\n",
    "from     scipy                  import  stats\n",
    "import   statsmodels.api        as     sm\n",
    "from     sklearn.preprocessing                 import   LabelEncoder\n",
    "from     statsmodels.stats.outliers_influence  import   variance_inflation_factor\n",
    "import   warnings\n",
    "\n",
    "from    sklearn.preprocessing    import   StandardScaler\n",
    "from    sklearn.preprocessing    import   MinMaxScaler\n",
    "from    sklearn.tree             import   DecisionTreeClassifier\n",
    "from    sklearn.metrics          import   accuracy_score, explained_variance_score\n",
    "\n",
    "from    sklearn.experimental     import enable_iterative_imputer\n",
    "from    sklearn.impute           import IterativeImputer\n",
    "from    sklearn.linear_model     import LinearRegression"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "ace8b1fa",
   "metadata": {},
   "outputs": [],
   "source": [
    "from   sklearn.feature_selection   import  RFECV\n",
    "from   sklearn.feature_selection   import  RFE\n",
    "from   sklearn                     import  metrics, preprocessing\n",
    "from   sklearn.pipeline            import  Pipeline\n",
    "from   sklearn.model_selection     import  cross_val_score, RepeatedStratifiedKFold, train_test_split\n",
    " \n",
    "from   sklearn.ensemble            import  RandomForestClassifier\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "de8b10f4",
   "metadata": {},
   "outputs": [],
   "source": [
    "import  os\n",
    "\n",
    "folder =  r'E:\\DSE-FT-B-JUN24-G2'\n",
    "pd.set_option('display.max_columns', None)\n",
    "os.chdir(folder)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "f50dbcd2",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'E:\\\\DSE-FT-B-JUN24-G2'"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "os.getcwd()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5e9b5e3c",
   "metadata": {},
   "source": [
    "#### Function to calculate count and percentage of missing values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "367be2e6",
   "metadata": {},
   "outputs": [],
   "source": [
    "def missing_zero_values_table(df):\n",
    "        zero_val = (df == 0.00).astype(int).sum(axis=0)\n",
    "        mis_val = df.isnull().sum()\n",
    "        mis_val_percent = 100 * df.isnull().sum() / len(df)\n",
    "        mz_table = pd.concat([zero_val, mis_val, mis_val_percent], axis=1)\n",
    "        mz_table = mz_table.rename(\n",
    "        columns = {0 : 'Zero Values', 1 : 'Missing Values', 2 : '% of Total Values'})\n",
    "        mz_table['Total Zero & Missing Values'] = mz_table['Zero Values'] + mz_table['Missing Values']\n",
    "        mz_table['% Total Zero & Missing Values'] = 100 * mz_table['Total Zero & Missing Values'] / len(df)\n",
    "        mz_table['Data Type'] = df.dtypes\n",
    "        mz_table = mz_table[\n",
    "            mz_table.iloc[:,1] != 0].sort_values(\n",
    "        '% of Total Values', ascending=False).round(1)\n",
    "        print (\"Your selected dataframe has \" + str(df.shape[1]) + \" columns and \" + str(df.shape[0]) + \" Rows.\\n\"      \n",
    "            \"There are \" + str(mz_table.shape[0]) +\n",
    "              \" columns that have missing values.\")\n",
    "        return mz_table"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "67379481",
   "metadata": {},
   "outputs": [],
   "source": [
    "def identify_outliers(df, var):\n",
    "    # Interquartile Range (IQR)\n",
    "\n",
    "    # Calculate the upper and lower limits\n",
    "    Q1                     =    df[var].quantile(0.25)\n",
    "    Q3                     =    df[var].quantile(0.75)\n",
    "    IQR                    =    Q3 - Q1  \n",
    "    low                    =    Q1 - (1.5 * IQR)\n",
    "    \n",
    "    lower                  =    abs(Q1 - 1.5 * IQR)\n",
    "    upper                  =    abs(Q3 + (1.5 * IQR))\n",
    "    outlier_upper          =    df[df[var] > upper].count()[1]\n",
    "    outlier_lower          =    df[df[var] < low].count()[1]\n",
    "    \n",
    "    m                      =    np.min(df[var])\n",
    "    mX                     =    np.max(df[var])\n",
    "    Zero_l                 =    df.loc[df[var]==0,var].sum()\n",
    "    \n",
    "    print(\"\\nFeature :{} Lower_quartile: {} ,Upper_quartile: {} \".format(var,lower, upper))\n",
    "    print(\"\\nMax value: {} Minimum value: {}\".format(mX, m))\n",
    "    print(\"\\nGreater than UpperIQR : {}, Less than lower_IQR : {} ,  zero_count:  {}\".format(outlier_upper,outlier_lower,Zero_l))\n",
    "    \n",
    "    outliers_present       =   outlier_upper > 0 or outlier_lower > 0\n",
    "    if outliers_present:\n",
    "       print(\"\\nVariable {} has outliers\".format(var))\n",
    "    else:\n",
    "       print(\"\\nVariable {} has NO outliers\".format(var))        \n",
    "    df.boxplot(column = [var])\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "680cc2ac",
   "metadata": {},
   "outputs": [],
   "source": [
    "def chk_ttest(var):\n",
    "    g1           =    df.loc[df['Exited'] == 0, var]\n",
    "    g2           =    df.loc[df['Exited'] == 1, var]    \n",
    "    stat, pval   =    stats.ttest_ind(g1, g2, equal_var = False)\n",
    "    txt          =    \"\\nThere is No statistically significant difference between the mean values of two groups of the Variable, {} \".format(var)\n",
    "    if pval < 0.05:\n",
    "       txt       =    \"\\nThere is a statistically significant difference between the mean values of two groups of the Variable, {} \".format(var)\n",
    "    \n",
    "    print(txt)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "82b64a10",
   "metadata": {},
   "outputs": [],
   "source": [
    "def chk_chisq(i, var):\n",
    "    print(\"\\n{0}: Variable, {1}\".format(i, var))\n",
    "    crosstab                                    = pd.crosstab(df[var], df['Exited'])\n",
    "    res                                         = stats.chi2_contingency(crosstab)\n",
    "    txt                                         =  \"Independent Variable and Target variable are independent\"\n",
    "    print(\"\\nThe important assumption: No more than 20% of the cells have and expected cell count < 5\")\n",
    "    print(\"\\nThis can be checked by looking at the expected frequency table.\")\n",
    "    print(res)\n",
    "    # Calculate the percentage of cells with expected counts less than 5\\\n",
    "    expected                =  res[3]\n",
    "    percentage_low_expected = (expected < 5).sum().sum() / (expected.shape[0] * expected.shape[1]) * 100\n",
    "    print(\"\\n\")\n",
    "    print(f\"Percentage of cells with expected counts less than 5: {percentage_low_expected:.2f}%\")\n",
    "    \n",
    "    if res[1] < 0.05:\n",
    "        txt                                     =  \"Independent Variable,{} and Target variable are dependent\".format(var)    \n",
    " \n",
    "    print(\"\\n{}\".format(txt))   "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "80a34e62",
   "metadata": {},
   "outputs": [],
   "source": [
    "def  hist_numerical(var, df):\n",
    "    mean_mu   =   df[var].mean()\n",
    "    ttile     =  \"Histogram for the variable: {}\".format(var)\n",
    "    mu_txt    =  r'$\\mu = $' + str(round(mean_mu,1)) \n",
    "    # Histogram\n",
    "    sns.histplot(x= df[var], kde = True)\n",
    "    plt.title(ttile)\n",
    "    plt.xticks(rotation = 90)\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "1c64cef2",
   "metadata": {},
   "outputs": [],
   "source": [
    "def  draw_countplot(df, i, var):\n",
    "     print(\"\\n{0}: Variable, {1}\".format(i, var))\n",
    "     x         =  df[var]\n",
    "     y         =  df['Exited']\n",
    "     ttile     =  \"Bar Chart for the variable: {}\".format(var) \n",
    "     plt.title(ttile)    \n",
    "     sns.countplot(x = x, hue = y, data = df)\n",
    "     plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "06d04aa5",
   "metadata": {},
   "outputs": [],
   "source": [
    "def draw_boxplot1(df, i, var):\n",
    "    print(\"\\n{0}: Variable, {1}\".format(i, var))\n",
    "    titlex  =  '\\n\\nBoxplot for ' + var + '\\n'\n",
    "    sns.catplot(data = df, x = 'Geography', y = var, col = 'Gender', hue = 'Exited', kind = 'box')\n",
    "    plt.xlabel(var)\n",
    "    plt.ylabel('Geography')\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "2f21c7c0",
   "metadata": {},
   "outputs": [],
   "source": [
    "def draw_boxplot2(df, i, var):\n",
    "    print(\"\\n{0}: Variable, {1}\".format(i, var))\n",
    "    titlex  =  '\\n\\nBoxplot for ' + var + '\\n'\n",
    "    sns.catplot(data = df, x = 'Card Type', y = var, col = 'Gender', hue = 'Exited', kind = 'box')\n",
    "    plt.xlabel(var)\n",
    "    plt.ylabel('Card Type')\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "4eb67d7e",
   "metadata": {},
   "outputs": [],
   "source": [
    "def pointbiserialr(df, col1, y):\n",
    "    x   =  df[col1]\n",
    "    stat, p    =  stats.pointbiserialr(y, x)\n",
    "    print('stat=%.3f, p=%.3f' % (stat, p))\n",
    "    if p > 0.05:\n",
    "     print('{} There is no correlation'.format(col1))\n",
    "    else:\n",
    "     print('{} There is a correlation'.format(col1))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "c9009524",
   "metadata": {},
   "outputs": [],
   "source": [
    "def  label_enc(df, col):\n",
    "     le        =  LabelEncoder()\n",
    "     colx      =  col + '_code'\n",
    "     df[colx]  = le.fit_transform(df[col])  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8bcc11bc-8c17-4881-a256-b2bb81cb26da",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "6059aa60",
   "metadata": {},
   "outputs": [],
   "source": [
    "### compare_models\n",
    "### ------------------------------------------------------------------------------------------\n",
    "\n",
    "\"\"\"\n",
    "9) Function Name: compare_models\n",
    "\n",
    "   Description: This **function** compares various algorithms on \n",
    "                 1) AUROC 2) Precision, 3) Recall\n",
    "   \n",
    "   Input: 1) splits for k fold \n",
    "          2) random seed number\n",
    "          3) Training data for predictor variables\n",
    "          4) Training data for target variable\n",
    "\n",
    "\n",
    "\n",
    "   Output: Model comparison on these metrics 1) AUROC 2) Metrics - Precision, Recall\n",
    "   \n",
    "\"\"\"\n",
    "def compare_models(n_splits, random_state, X_train, Y_train):  \n",
    "\n",
    "    ### To compare algorithms\n",
    "    \n",
    "    from    matplotlib                  import   pyplot\n",
    "    from    sklearn.model_selection     import   KFold\n",
    "    from    sklearn.model_selection     import   StratifiedKFold\n",
    "    from    sklearn.model_selection     import   cross_val_score\n",
    "    from    sklearn.linear_model        import   LogisticRegression\n",
    "    from    sklearn.tree                import   DecisionTreeClassifier\n",
    "    from    sklearn.neighbors           import   KNeighborsClassifier\n",
    "    from    sklearn.naive_bayes         import   GaussianNB\n",
    "    from    sklearn.ensemble            import   RandomForestClassifier\n",
    "    \n",
    "    ### Prepare models\n",
    "    \n",
    "    models  = []\n",
    "    models.append(('LR', LogisticRegression()))\n",
    "    models.append(('KNN', KNeighborsClassifier()))\n",
    "    models.append(('CART', DecisionTreeClassifier()))\n",
    "    models.append(('NB', GaussianNB()))\n",
    "    models.append(('RF', RandomForestClassifier()))\n",
    "    \n",
    "    \n",
    "    ### Evaluate model in turn\n",
    "    \n",
    "\n",
    "    scores_req =  ['roc_auc', 'precision', 'recall']\n",
    "   \n",
    "    print(\"\\n n_splits %d random_state %d\" % (n_splits, random_state)) \n",
    "    \n",
    "    for i in range(len(scores_req)):\n",
    "        results    =   []\n",
    "        names      =   []        \n",
    "        scoring    =  scores_req[i]\n",
    "        \n",
    "        print(scoring)\n",
    "        \n",
    "        for name, model in models:\n",
    "\n",
    "            ## kfold           =   KFold(n_splits = n_splits, random_state = 12345) \n",
    "            skf             =   StratifiedKFold(n_splits=n_splits, random_state = random_state, shuffle = True)\n",
    "            cv_results      =   cross_val_score(model, X_train, Y_train, cv = skf, scoring = scoring)\n",
    "            results.append(cv_results)\t\n",
    "            names.append(name)\n",
    "            msg = \"%s: %f (%f)\" % (name, cv_results.mean(), cv_results.std())\n",
    "            print(msg)\n",
    "        \n",
    "        ### Box plot algorithm comparison\n",
    "        \n",
    "        sub_title = 'Algorithm Comparison using ' + scoring\n",
    "        \n",
    "        fig = pyplot.figure()\n",
    "        fig.suptitle(sub_title)\n",
    "        ax  = fig.add_subplot(111)\n",
    "        pyplot.boxplot(results)\n",
    "        ax.set_xticklabels(names, rotation=90, ha='right')\n",
    "        pyplot.show()\n",
    "\n",
    "### ------------------------------------------------------------------------------------------"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "e205c5d7",
   "metadata": {},
   "outputs": [],
   "source": [
    "from statsmodels.stats.outliers_influence import variance_inflation_factor    \n",
    "\n",
    "def calculate_vif_(X, thresh = 5):\n",
    "    cols = X.columns\n",
    "    variables = np.arange(X.shape[1])\n",
    "    dropped=True\n",
    "    while dropped:\n",
    "        dropped=False\n",
    "        c = X[cols[variables]].values\n",
    "        vif = [variance_inflation_factor(c, ix) for ix in np.arange(c.shape[1])]\n",
    "\n",
    "        maxloc = vif.index(max(vif))\n",
    "        if max(vif) > thresh:\n",
    "            print('dropping \\'' + X[cols[variables]].columns[maxloc] + '\\' at index: ' + str(maxloc))\n",
    "            variables = np.delete(variables, maxloc)\n",
    "            dropped=True\n",
    "\n",
    "    print('Remaining variables:')\n",
    "    print(X.columns[variables])\n",
    "    ncdf = X[cols[variables]]\n",
    "    return ncdf"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "1e85f18a",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_significant_vars( lm, sig_level ):\n",
    "    \n",
    "    import pandas as pd\n",
    "    \n",
    "    var_p_vals_df         = pd.DataFrame( lm.pvalues )\n",
    "    var_p_vals_df['vars'] = var_p_vals_df.index\n",
    "    var_p_vals_df.columns = ['pvals', 'vars']\n",
    "    \n",
    "    return list( var_p_vals_df[var_p_vals_df.pvals <= sig_level]['vars'] )\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "d3fdf40c",
   "metadata": {},
   "outputs": [],
   "source": [
    "def check_linearity(x1, df, title, y1):\n",
    "    \n",
    "    import matplotlib.pyplot as plt\n",
    "    import seaborn           as sns\n",
    "    \n",
    "    sns.regplot(x = x1, y= y1, data= df, logistic= True).set_title(title)\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "dd9f2992",
   "metadata": {},
   "outputs": [],
   "source": [
    "def logit_reg(n_splits, random_state, X_train, X_test, Y_train, Y_test ):  \n",
    "\n",
    "    import statsmodels.api          as      sm\n",
    "    \n",
    "    from   sklearn.linear_model     import  LogisticRegression \n",
    "    from   sklearn.metrics          import  classification_report\n",
    "    from   sklearn.metrics          import  confusion_matrix\n",
    "    from   sklearn.model_selection  import  cross_val_score\n",
    "    \n",
    "    from   sklearn.model_selection  import  KFold \n",
    "    \n",
    "    model = LogisticRegression() \n",
    "\n",
    "    model.fit(X_train, Y_train) \n",
    "    predicted_train    = model.predict(X_train) \n",
    "    matrix             = confusion_matrix(Y_train, predicted_train)\n",
    "    print(\"\\nTraining Data\")\n",
    "    print(matrix)\n",
    "    draw_cm(Y_train, predicted_train )\n",
    "    \n",
    "    accuracy_train = model.score(X_train, Y_train) \n",
    "    print(\"Training Accuracy: %.3f%%\" % (accuracy_train * 100.0))\n",
    "\n",
    "    print(\"\\nTesting Data\")\n",
    "\n",
    "    predicted_testing  = model.predict(X_test) \n",
    "    matrix             = confusion_matrix(Y_test, predicted_testing)\n",
    "    print(matrix)\n",
    "    draw_cm(Y_test, predicted_testing)\n",
    "    \n",
    "    accuracy_test      = model.score(X_test, Y_test) \n",
    "    print(\"Test Accuracy: %.3f%%\" % (accuracy_test * 100.0))\n",
    "    \n",
    "\n",
    "    measures_train     = classification_report(Y_train, predicted_train) \n",
    "    print(\"\\nTraining data\")\n",
    "    print(measures_train) \n",
    "\n",
    "    measures_test      = classification_report(Y_test, predicted_testing) \n",
    "    print(\"\\nTesting data\")\n",
    "    print(measures_test) \n",
    "    \n",
    "    kfold              = KFold(n_splits = n_splits, shuffle = True, random_state = random_state)\n",
    "    scoring            = 'roc_auc' \n",
    "\n",
    "    auc_train          = cross_val_score(model, X_train, Y_train,  scoring=scoring) \n",
    "    print(\"\\nTraining data\")\n",
    "    draw_roc( Y_train, predicted_train)    \n",
    "\n",
    "\n",
    "    auc_test           = cross_val_score(model, X_test, Y_test,  scoring=scoring) \n",
    "    print(\"\\nTesting data\")    \n",
    "    draw_roc( Y_test, predicted_testing)\n",
    "    \n",
    "    print(\"\\nWith K fold cross validation\")\n",
    "    \n",
    "    scoring           = 'accuracy'\n",
    "    print(\"\\nScoring:  %s\" %scoring)\n",
    "    cv_accuracy_train = cross_val_score(model, X_train, Y_train, cv = kfold,  scoring=scoring)\n",
    "    print(cv_accuracy_train)\n",
    "    print(\"\\nAccuracy: %.3f (%.3f)\" % (cv_accuracy_train.mean(), cv_accuracy_train.std()))\n",
    "    \n",
    "    scoring           = 'precision'\n",
    "    print(\"\\nScoring:  %s\" %scoring)    \n",
    "    cv_precision_train = cross_val_score(model, X_train, Y_train, cv = kfold,  scoring=scoring)\n",
    "    print(cv_precision_train)\n",
    "    print(\"\\nPrecision: %.3f (%.3f)\" % (cv_precision_train.mean(), cv_precision_train.std()))\n",
    "  \n",
    "    scoring           = 'recall'\n",
    "    print(\"\\nScoring:  %s\" %scoring)\n",
    "    \n",
    "    cv_recall_train   = cross_val_score(model, X_train, Y_train, cv = kfold,  scoring=scoring)\n",
    "    print(cv_recall_train)\n",
    "    print(\"\\nRecall: %.3f (%.3f)\" % (cv_recall_train.mean(), cv_recall_train.std()))\n",
    "    \n",
    "    scoring           = 'roc_auc'\n",
    "    print(\"\\nScoring:  %s\" %scoring)    \n",
    "    cv_roc_auc_train = cross_val_score(model, X_train, Y_train, cv = kfold,  scoring=scoring)\n",
    "    print(cv_roc_auc_train)\n",
    "    print(\"\\nAUROC: %.3f (%.3f)\" % (cv_roc_auc_train.mean(), cv_roc_auc_train.std()))\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "390df596",
   "metadata": {},
   "outputs": [],
   "source": [
    "def draw_cm( actual, predicted ):\n",
    "    \n",
    "    import matplotlib.pyplot as   plt\n",
    "    import sklearn.metrics   as   metrics\n",
    "    import seaborn           as   sns\n",
    "    \n",
    "    cm = metrics.confusion_matrix( actual, predicted )\n",
    "    sns.heatmap(cm, annot=True, fmt='.2f', xticklabels = [\"Yes\", \"No\"] , yticklabels = [\"Yes\", \"No\"] )\n",
    "    plt.ylabel('True label')\n",
    "    plt.xlabel('Predicted label')\n",
    "    plt.show()   "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "b19dfb15",
   "metadata": {},
   "outputs": [],
   "source": [
    "def draw_roc( actual, probs ):\n",
    "\n",
    "    import pandas            as   pd\n",
    "    import numpy             as   np\n",
    "    import seaborn           as   sns\n",
    "    import matplotlib.pyplot as   plt\n",
    "    import sklearn.metrics   as   metrics\n",
    "    import seaborn           as   sns\n",
    "    \n",
    "    fpr, tpr, thresholds = metrics.roc_curve( actual, probs,\n",
    "    drop_intermediate = False )\n",
    "    auc_score = metrics.roc_auc_score( actual, probs )\n",
    "    plt.figure(figsize=(6, 4))\n",
    "    plt.plot( fpr, tpr, label='ROC curve (area = %0.2f)' % auc_score )\n",
    "    plt.plot([0, 1], [0, 1], 'k--')\n",
    "    plt.xlim([0.0, 1.0])\n",
    "    plt.ylim([0.0, 1.05])\n",
    "    plt.xlabel('False Positive Rate or [1 - True Negative Rate]')\n",
    "    plt.ylabel('True Positive Rate')\n",
    "    plt.title('Receiver operating characteristic curve')\n",
    "    plt.legend(loc=\"lower right\")\n",
    "    plt.show()\n",
    "    return fpr, tpr, thresholds"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bcd72d07",
   "metadata": {},
   "source": [
    "### 2) Read data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "09ec2388-322a-4295-a3b9-bad9d4f80aca",
   "metadata": {},
   "outputs": [],
   "source": [
    "file   =  './Data/DSEFTB-JUN24-G2-EDA-Data_2024_11_23_18_25_55.csv'\n",
    "df     =  pd.read_csv(file, encoding = 'Latin-1')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "bc18d3ce",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>gender</th>\n",
       "      <th>age</th>\n",
       "      <th>admission_type_id</th>\n",
       "      <th>discharge_disposition_id</th>\n",
       "      <th>admission_source_id</th>\n",
       "      <th>time_in_hospital</th>\n",
       "      <th>num_lab_procedures</th>\n",
       "      <th>num_procedures</th>\n",
       "      <th>num_medications</th>\n",
       "      <th>number_outpatient</th>\n",
       "      <th>number_emergency</th>\n",
       "      <th>number_inpatient</th>\n",
       "      <th>number_diagnoses</th>\n",
       "      <th>max_glu_serum</th>\n",
       "      <th>A1Cresult</th>\n",
       "      <th>metformin</th>\n",
       "      <th>repaglinide</th>\n",
       "      <th>nateglinide</th>\n",
       "      <th>chlorpropamide</th>\n",
       "      <th>glimepiride</th>\n",
       "      <th>acetohexamide</th>\n",
       "      <th>glipizide</th>\n",
       "      <th>glyburide</th>\n",
       "      <th>tolbutamide</th>\n",
       "      <th>pioglitazone</th>\n",
       "      <th>rosiglitazone</th>\n",
       "      <th>acarbose</th>\n",
       "      <th>miglitol</th>\n",
       "      <th>troglitazone</th>\n",
       "      <th>tolazamide</th>\n",
       "      <th>examide</th>\n",
       "      <th>citoglipton</th>\n",
       "      <th>insulin</th>\n",
       "      <th>glyburide-metformin</th>\n",
       "      <th>glipizide-metformin</th>\n",
       "      <th>glimepiride-pioglitazone</th>\n",
       "      <th>metformin-rosiglitazone</th>\n",
       "      <th>metformin-pioglitazone</th>\n",
       "      <th>change</th>\n",
       "      <th>diabetesMed</th>\n",
       "      <th>Target</th>\n",
       "      <th>race</th>\n",
       "      <th>payer_code</th>\n",
       "      <th>medical_specialty</th>\n",
       "      <th>diag_1</th>\n",
       "      <th>diag_2</th>\n",
       "      <th>diag_3</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>Female</td>\n",
       "      <td>[0-10)</td>\n",
       "      <td>6</td>\n",
       "      <td>25</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>41</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>Nil</td>\n",
       "      <td>Nil</td>\n",
       "      <td>No</td>\n",
       "      <td>No</td>\n",
       "      <td>No</td>\n",
       "      <td>No</td>\n",
       "      <td>No</td>\n",
       "      <td>No</td>\n",
       "      <td>No</td>\n",
       "      <td>No</td>\n",
       "      <td>No</td>\n",
       "      <td>No</td>\n",
       "      <td>No</td>\n",
       "      <td>No</td>\n",
       "      <td>No</td>\n",
       "      <td>No</td>\n",
       "      <td>No</td>\n",
       "      <td>No</td>\n",
       "      <td>No</td>\n",
       "      <td>No</td>\n",
       "      <td>No</td>\n",
       "      <td>No</td>\n",
       "      <td>No</td>\n",
       "      <td>No</td>\n",
       "      <td>No</td>\n",
       "      <td>No</td>\n",
       "      <td>No</td>\n",
       "      <td>0</td>\n",
       "      <td>Caucasian</td>\n",
       "      <td>MD</td>\n",
       "      <td>Pediatrics-Endocrinology</td>\n",
       "      <td>250.83</td>\n",
       "      <td>345</td>\n",
       "      <td>224</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>Female</td>\n",
       "      <td>[10-20)</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>7</td>\n",
       "      <td>3</td>\n",
       "      <td>59</td>\n",
       "      <td>0</td>\n",
       "      <td>18</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>9</td>\n",
       "      <td>Nil</td>\n",
       "      <td>Nil</td>\n",
       "      <td>No</td>\n",
       "      <td>No</td>\n",
       "      <td>No</td>\n",
       "      <td>No</td>\n",
       "      <td>No</td>\n",
       "      <td>No</td>\n",
       "      <td>No</td>\n",
       "      <td>No</td>\n",
       "      <td>No</td>\n",
       "      <td>No</td>\n",
       "      <td>No</td>\n",
       "      <td>No</td>\n",
       "      <td>No</td>\n",
       "      <td>No</td>\n",
       "      <td>No</td>\n",
       "      <td>No</td>\n",
       "      <td>No</td>\n",
       "      <td>Up</td>\n",
       "      <td>No</td>\n",
       "      <td>No</td>\n",
       "      <td>No</td>\n",
       "      <td>No</td>\n",
       "      <td>No</td>\n",
       "      <td>Ch</td>\n",
       "      <td>Yes</td>\n",
       "      <td>0</td>\n",
       "      <td>Caucasian</td>\n",
       "      <td>MC</td>\n",
       "      <td>Hematology/Oncology</td>\n",
       "      <td>276</td>\n",
       "      <td>250.01</td>\n",
       "      <td>121</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>Female</td>\n",
       "      <td>[20-30)</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>7</td>\n",
       "      <td>2</td>\n",
       "      <td>11</td>\n",
       "      <td>5</td>\n",
       "      <td>13</td>\n",
       "      <td>2</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>6</td>\n",
       "      <td>Nil</td>\n",
       "      <td>Nil</td>\n",
       "      <td>No</td>\n",
       "      <td>No</td>\n",
       "      <td>No</td>\n",
       "      <td>No</td>\n",
       "      <td>No</td>\n",
       "      <td>No</td>\n",
       "      <td>Steady</td>\n",
       "      <td>No</td>\n",
       "      <td>No</td>\n",
       "      <td>No</td>\n",
       "      <td>No</td>\n",
       "      <td>No</td>\n",
       "      <td>No</td>\n",
       "      <td>No</td>\n",
       "      <td>No</td>\n",
       "      <td>No</td>\n",
       "      <td>No</td>\n",
       "      <td>No</td>\n",
       "      <td>No</td>\n",
       "      <td>No</td>\n",
       "      <td>No</td>\n",
       "      <td>No</td>\n",
       "      <td>No</td>\n",
       "      <td>No</td>\n",
       "      <td>Yes</td>\n",
       "      <td>0</td>\n",
       "      <td>AfricanAmerican</td>\n",
       "      <td>HM</td>\n",
       "      <td>ObstetricsandGynecology</td>\n",
       "      <td>648</td>\n",
       "      <td>250</td>\n",
       "      <td>766</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>Male</td>\n",
       "      <td>[30-40)</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>7</td>\n",
       "      <td>2</td>\n",
       "      <td>44</td>\n",
       "      <td>1</td>\n",
       "      <td>16</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>7</td>\n",
       "      <td>Nil</td>\n",
       "      <td>Nil</td>\n",
       "      <td>No</td>\n",
       "      <td>No</td>\n",
       "      <td>No</td>\n",
       "      <td>No</td>\n",
       "      <td>No</td>\n",
       "      <td>No</td>\n",
       "      <td>No</td>\n",
       "      <td>No</td>\n",
       "      <td>No</td>\n",
       "      <td>No</td>\n",
       "      <td>No</td>\n",
       "      <td>No</td>\n",
       "      <td>No</td>\n",
       "      <td>No</td>\n",
       "      <td>No</td>\n",
       "      <td>No</td>\n",
       "      <td>No</td>\n",
       "      <td>Up</td>\n",
       "      <td>No</td>\n",
       "      <td>No</td>\n",
       "      <td>No</td>\n",
       "      <td>No</td>\n",
       "      <td>No</td>\n",
       "      <td>Ch</td>\n",
       "      <td>Yes</td>\n",
       "      <td>0</td>\n",
       "      <td>Caucasian</td>\n",
       "      <td>MC</td>\n",
       "      <td>InternalMedicine</td>\n",
       "      <td>8</td>\n",
       "      <td>250.43</td>\n",
       "      <td>248</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>Male</td>\n",
       "      <td>[40-50)</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>7</td>\n",
       "      <td>1</td>\n",
       "      <td>51</td>\n",
       "      <td>0</td>\n",
       "      <td>8</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>5</td>\n",
       "      <td>Nil</td>\n",
       "      <td>Nil</td>\n",
       "      <td>No</td>\n",
       "      <td>No</td>\n",
       "      <td>No</td>\n",
       "      <td>No</td>\n",
       "      <td>No</td>\n",
       "      <td>No</td>\n",
       "      <td>Steady</td>\n",
       "      <td>No</td>\n",
       "      <td>No</td>\n",
       "      <td>No</td>\n",
       "      <td>No</td>\n",
       "      <td>No</td>\n",
       "      <td>No</td>\n",
       "      <td>No</td>\n",
       "      <td>No</td>\n",
       "      <td>No</td>\n",
       "      <td>No</td>\n",
       "      <td>Steady</td>\n",
       "      <td>No</td>\n",
       "      <td>No</td>\n",
       "      <td>No</td>\n",
       "      <td>No</td>\n",
       "      <td>No</td>\n",
       "      <td>Ch</td>\n",
       "      <td>Yes</td>\n",
       "      <td>0</td>\n",
       "      <td>Caucasian</td>\n",
       "      <td>MD</td>\n",
       "      <td>Hematology/Oncology</td>\n",
       "      <td>197</td>\n",
       "      <td>157</td>\n",
       "      <td>86</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   gender      age  admission_type_id  discharge_disposition_id  \\\n",
       "0  Female   [0-10)                  6                        25   \n",
       "1  Female  [10-20)                  1                         1   \n",
       "2  Female  [20-30)                  1                         1   \n",
       "3    Male  [30-40)                  1                         1   \n",
       "4    Male  [40-50)                  1                         1   \n",
       "\n",
       "   admission_source_id  time_in_hospital  num_lab_procedures  num_procedures  \\\n",
       "0                    1                 1                  41               0   \n",
       "1                    7                 3                  59               0   \n",
       "2                    7                 2                  11               5   \n",
       "3                    7                 2                  44               1   \n",
       "4                    7                 1                  51               0   \n",
       "\n",
       "   num_medications  number_outpatient  number_emergency  number_inpatient  \\\n",
       "0                1                  0                 0                 0   \n",
       "1               18                  0                 0                 0   \n",
       "2               13                  2                 0                 1   \n",
       "3               16                  0                 0                 0   \n",
       "4                8                  0                 0                 0   \n",
       "\n",
       "   number_diagnoses max_glu_serum A1Cresult metformin repaglinide nateglinide  \\\n",
       "0                 1           Nil       Nil        No          No          No   \n",
       "1                 9           Nil       Nil        No          No          No   \n",
       "2                 6           Nil       Nil        No          No          No   \n",
       "3                 7           Nil       Nil        No          No          No   \n",
       "4                 5           Nil       Nil        No          No          No   \n",
       "\n",
       "  chlorpropamide glimepiride acetohexamide glipizide glyburide tolbutamide  \\\n",
       "0             No          No            No        No        No          No   \n",
       "1             No          No            No        No        No          No   \n",
       "2             No          No            No    Steady        No          No   \n",
       "3             No          No            No        No        No          No   \n",
       "4             No          No            No    Steady        No          No   \n",
       "\n",
       "  pioglitazone rosiglitazone acarbose miglitol troglitazone tolazamide  \\\n",
       "0           No            No       No       No           No         No   \n",
       "1           No            No       No       No           No         No   \n",
       "2           No            No       No       No           No         No   \n",
       "3           No            No       No       No           No         No   \n",
       "4           No            No       No       No           No         No   \n",
       "\n",
       "  examide citoglipton insulin glyburide-metformin glipizide-metformin  \\\n",
       "0      No          No      No                  No                  No   \n",
       "1      No          No      Up                  No                  No   \n",
       "2      No          No      No                  No                  No   \n",
       "3      No          No      Up                  No                  No   \n",
       "4      No          No  Steady                  No                  No   \n",
       "\n",
       "  glimepiride-pioglitazone metformin-rosiglitazone metformin-pioglitazone  \\\n",
       "0                       No                      No                     No   \n",
       "1                       No                      No                     No   \n",
       "2                       No                      No                     No   \n",
       "3                       No                      No                     No   \n",
       "4                       No                      No                     No   \n",
       "\n",
       "  change diabetesMed  Target             race payer_code  \\\n",
       "0     No          No       0        Caucasian         MD   \n",
       "1     Ch         Yes       0        Caucasian         MC   \n",
       "2     No         Yes       0  AfricanAmerican         HM   \n",
       "3     Ch         Yes       0        Caucasian         MC   \n",
       "4     Ch         Yes       0        Caucasian         MD   \n",
       "\n",
       "          medical_specialty  diag_1  diag_2  diag_3  \n",
       "0  Pediatrics-Endocrinology  250.83     345     224  \n",
       "1       Hematology/Oncology     276  250.01     121  \n",
       "2   ObstetricsandGynecology     648     250     766  \n",
       "3          InternalMedicine       8  250.43     248  \n",
       "4       Hematology/Oncology     197     157      86  "
      ]
     },
     "execution_count": 30,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "e8c1f4a8-a6ab-4935-bfdc-5a51bf59fb4d",
   "metadata": {},
   "outputs": [],
   "source": [
    "df['diag_1']    =   df['diag_1'].astype('category')\n",
    "df['diag_2']    =   df['diag_2'].astype('category')\n",
    "df['diag_3']    =   df['diag_3'].astype('category')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "58aaa732",
   "metadata": {},
   "source": [
    "### 3) Understanding Dataset and Domain"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d411da7d",
   "metadata": {},
   "source": [
    "### Variable categorization (count of numeric and categorical)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "247e4cad",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'pandas.core.frame.DataFrame'>\n",
      "RangeIndex: 101766 entries, 0 to 101765\n",
      "Data columns (total 47 columns):\n",
      " #   Column                    Non-Null Count   Dtype   \n",
      "---  ------                    --------------   -----   \n",
      " 0   gender                    101766 non-null  object  \n",
      " 1   age                       101766 non-null  object  \n",
      " 2   admission_type_id         101766 non-null  int64   \n",
      " 3   discharge_disposition_id  101766 non-null  int64   \n",
      " 4   admission_source_id       101766 non-null  int64   \n",
      " 5   time_in_hospital          101766 non-null  int64   \n",
      " 6   num_lab_procedures        101766 non-null  int64   \n",
      " 7   num_procedures            101766 non-null  int64   \n",
      " 8   num_medications           101766 non-null  int64   \n",
      " 9   number_outpatient         101766 non-null  int64   \n",
      " 10  number_emergency          101766 non-null  int64   \n",
      " 11  number_inpatient          101766 non-null  int64   \n",
      " 12  number_diagnoses          101766 non-null  int64   \n",
      " 13  max_glu_serum             101766 non-null  object  \n",
      " 14  A1Cresult                 101766 non-null  object  \n",
      " 15  metformin                 101766 non-null  object  \n",
      " 16  repaglinide               101766 non-null  object  \n",
      " 17  nateglinide               101766 non-null  object  \n",
      " 18  chlorpropamide            101766 non-null  object  \n",
      " 19  glimepiride               101766 non-null  object  \n",
      " 20  acetohexamide             101766 non-null  object  \n",
      " 21  glipizide                 101766 non-null  object  \n",
      " 22  glyburide                 101766 non-null  object  \n",
      " 23  tolbutamide               101766 non-null  object  \n",
      " 24  pioglitazone              101766 non-null  object  \n",
      " 25  rosiglitazone             101766 non-null  object  \n",
      " 26  acarbose                  101766 non-null  object  \n",
      " 27  miglitol                  101766 non-null  object  \n",
      " 28  troglitazone              101766 non-null  object  \n",
      " 29  tolazamide                101766 non-null  object  \n",
      " 30  examide                   101766 non-null  object  \n",
      " 31  citoglipton               101766 non-null  object  \n",
      " 32  insulin                   101766 non-null  object  \n",
      " 33  glyburide-metformin       101766 non-null  object  \n",
      " 34  glipizide-metformin       101766 non-null  object  \n",
      " 35  glimepiride-pioglitazone  101766 non-null  object  \n",
      " 36  metformin-rosiglitazone   101766 non-null  object  \n",
      " 37  metformin-pioglitazone    101766 non-null  object  \n",
      " 38  change                    101766 non-null  object  \n",
      " 39  diabetesMed               101766 non-null  object  \n",
      " 40  Target                    101766 non-null  int64   \n",
      " 41  race                      101766 non-null  object  \n",
      " 42  payer_code                101766 non-null  object  \n",
      " 43  medical_specialty         101766 non-null  object  \n",
      " 44  diag_1                    101766 non-null  category\n",
      " 45  diag_2                    101766 non-null  category\n",
      " 46  diag_3                    101766 non-null  category\n",
      "dtypes: category(3), int64(12), object(32)\n",
      "memory usage: 34.8+ MB\n"
     ]
    }
   ],
   "source": [
    "df.info()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "id": "e5906461",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>gender</th>\n",
       "      <th>age</th>\n",
       "      <th>max_glu_serum</th>\n",
       "      <th>A1Cresult</th>\n",
       "      <th>metformin</th>\n",
       "      <th>repaglinide</th>\n",
       "      <th>nateglinide</th>\n",
       "      <th>chlorpropamide</th>\n",
       "      <th>glimepiride</th>\n",
       "      <th>acetohexamide</th>\n",
       "      <th>glipizide</th>\n",
       "      <th>glyburide</th>\n",
       "      <th>tolbutamide</th>\n",
       "      <th>pioglitazone</th>\n",
       "      <th>rosiglitazone</th>\n",
       "      <th>acarbose</th>\n",
       "      <th>miglitol</th>\n",
       "      <th>troglitazone</th>\n",
       "      <th>tolazamide</th>\n",
       "      <th>examide</th>\n",
       "      <th>citoglipton</th>\n",
       "      <th>insulin</th>\n",
       "      <th>glyburide-metformin</th>\n",
       "      <th>glipizide-metformin</th>\n",
       "      <th>glimepiride-pioglitazone</th>\n",
       "      <th>metformin-rosiglitazone</th>\n",
       "      <th>metformin-pioglitazone</th>\n",
       "      <th>change</th>\n",
       "      <th>diabetesMed</th>\n",
       "      <th>race</th>\n",
       "      <th>payer_code</th>\n",
       "      <th>medical_specialty</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>Female</td>\n",
       "      <td>[0-10)</td>\n",
       "      <td>Nil</td>\n",
       "      <td>Nil</td>\n",
       "      <td>No</td>\n",
       "      <td>No</td>\n",
       "      <td>No</td>\n",
       "      <td>No</td>\n",
       "      <td>No</td>\n",
       "      <td>No</td>\n",
       "      <td>No</td>\n",
       "      <td>No</td>\n",
       "      <td>No</td>\n",
       "      <td>No</td>\n",
       "      <td>No</td>\n",
       "      <td>No</td>\n",
       "      <td>No</td>\n",
       "      <td>No</td>\n",
       "      <td>No</td>\n",
       "      <td>No</td>\n",
       "      <td>No</td>\n",
       "      <td>No</td>\n",
       "      <td>No</td>\n",
       "      <td>No</td>\n",
       "      <td>No</td>\n",
       "      <td>No</td>\n",
       "      <td>No</td>\n",
       "      <td>No</td>\n",
       "      <td>No</td>\n",
       "      <td>Caucasian</td>\n",
       "      <td>MD</td>\n",
       "      <td>Pediatrics-Endocrinology</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>Female</td>\n",
       "      <td>[10-20)</td>\n",
       "      <td>Nil</td>\n",
       "      <td>Nil</td>\n",
       "      <td>No</td>\n",
       "      <td>No</td>\n",
       "      <td>No</td>\n",
       "      <td>No</td>\n",
       "      <td>No</td>\n",
       "      <td>No</td>\n",
       "      <td>No</td>\n",
       "      <td>No</td>\n",
       "      <td>No</td>\n",
       "      <td>No</td>\n",
       "      <td>No</td>\n",
       "      <td>No</td>\n",
       "      <td>No</td>\n",
       "      <td>No</td>\n",
       "      <td>No</td>\n",
       "      <td>No</td>\n",
       "      <td>No</td>\n",
       "      <td>Up</td>\n",
       "      <td>No</td>\n",
       "      <td>No</td>\n",
       "      <td>No</td>\n",
       "      <td>No</td>\n",
       "      <td>No</td>\n",
       "      <td>Ch</td>\n",
       "      <td>Yes</td>\n",
       "      <td>Caucasian</td>\n",
       "      <td>MC</td>\n",
       "      <td>Hematology/Oncology</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>Female</td>\n",
       "      <td>[20-30)</td>\n",
       "      <td>Nil</td>\n",
       "      <td>Nil</td>\n",
       "      <td>No</td>\n",
       "      <td>No</td>\n",
       "      <td>No</td>\n",
       "      <td>No</td>\n",
       "      <td>No</td>\n",
       "      <td>No</td>\n",
       "      <td>Steady</td>\n",
       "      <td>No</td>\n",
       "      <td>No</td>\n",
       "      <td>No</td>\n",
       "      <td>No</td>\n",
       "      <td>No</td>\n",
       "      <td>No</td>\n",
       "      <td>No</td>\n",
       "      <td>No</td>\n",
       "      <td>No</td>\n",
       "      <td>No</td>\n",
       "      <td>No</td>\n",
       "      <td>No</td>\n",
       "      <td>No</td>\n",
       "      <td>No</td>\n",
       "      <td>No</td>\n",
       "      <td>No</td>\n",
       "      <td>No</td>\n",
       "      <td>Yes</td>\n",
       "      <td>AfricanAmerican</td>\n",
       "      <td>HM</td>\n",
       "      <td>ObstetricsandGynecology</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>Male</td>\n",
       "      <td>[30-40)</td>\n",
       "      <td>Nil</td>\n",
       "      <td>Nil</td>\n",
       "      <td>No</td>\n",
       "      <td>No</td>\n",
       "      <td>No</td>\n",
       "      <td>No</td>\n",
       "      <td>No</td>\n",
       "      <td>No</td>\n",
       "      <td>No</td>\n",
       "      <td>No</td>\n",
       "      <td>No</td>\n",
       "      <td>No</td>\n",
       "      <td>No</td>\n",
       "      <td>No</td>\n",
       "      <td>No</td>\n",
       "      <td>No</td>\n",
       "      <td>No</td>\n",
       "      <td>No</td>\n",
       "      <td>No</td>\n",
       "      <td>Up</td>\n",
       "      <td>No</td>\n",
       "      <td>No</td>\n",
       "      <td>No</td>\n",
       "      <td>No</td>\n",
       "      <td>No</td>\n",
       "      <td>Ch</td>\n",
       "      <td>Yes</td>\n",
       "      <td>Caucasian</td>\n",
       "      <td>MC</td>\n",
       "      <td>InternalMedicine</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>Male</td>\n",
       "      <td>[40-50)</td>\n",
       "      <td>Nil</td>\n",
       "      <td>Nil</td>\n",
       "      <td>No</td>\n",
       "      <td>No</td>\n",
       "      <td>No</td>\n",
       "      <td>No</td>\n",
       "      <td>No</td>\n",
       "      <td>No</td>\n",
       "      <td>Steady</td>\n",
       "      <td>No</td>\n",
       "      <td>No</td>\n",
       "      <td>No</td>\n",
       "      <td>No</td>\n",
       "      <td>No</td>\n",
       "      <td>No</td>\n",
       "      <td>No</td>\n",
       "      <td>No</td>\n",
       "      <td>No</td>\n",
       "      <td>No</td>\n",
       "      <td>Steady</td>\n",
       "      <td>No</td>\n",
       "      <td>No</td>\n",
       "      <td>No</td>\n",
       "      <td>No</td>\n",
       "      <td>No</td>\n",
       "      <td>Ch</td>\n",
       "      <td>Yes</td>\n",
       "      <td>Caucasian</td>\n",
       "      <td>MD</td>\n",
       "      <td>Hematology/Oncology</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   gender      age max_glu_serum A1Cresult metformin repaglinide nateglinide  \\\n",
       "0  Female   [0-10)           Nil       Nil        No          No          No   \n",
       "1  Female  [10-20)           Nil       Nil        No          No          No   \n",
       "2  Female  [20-30)           Nil       Nil        No          No          No   \n",
       "3    Male  [30-40)           Nil       Nil        No          No          No   \n",
       "4    Male  [40-50)           Nil       Nil        No          No          No   \n",
       "\n",
       "  chlorpropamide glimepiride acetohexamide glipizide glyburide tolbutamide  \\\n",
       "0             No          No            No        No        No          No   \n",
       "1             No          No            No        No        No          No   \n",
       "2             No          No            No    Steady        No          No   \n",
       "3             No          No            No        No        No          No   \n",
       "4             No          No            No    Steady        No          No   \n",
       "\n",
       "  pioglitazone rosiglitazone acarbose miglitol troglitazone tolazamide  \\\n",
       "0           No            No       No       No           No         No   \n",
       "1           No            No       No       No           No         No   \n",
       "2           No            No       No       No           No         No   \n",
       "3           No            No       No       No           No         No   \n",
       "4           No            No       No       No           No         No   \n",
       "\n",
       "  examide citoglipton insulin glyburide-metformin glipizide-metformin  \\\n",
       "0      No          No      No                  No                  No   \n",
       "1      No          No      Up                  No                  No   \n",
       "2      No          No      No                  No                  No   \n",
       "3      No          No      Up                  No                  No   \n",
       "4      No          No  Steady                  No                  No   \n",
       "\n",
       "  glimepiride-pioglitazone metformin-rosiglitazone metformin-pioglitazone  \\\n",
       "0                       No                      No                     No   \n",
       "1                       No                      No                     No   \n",
       "2                       No                      No                     No   \n",
       "3                       No                      No                     No   \n",
       "4                       No                      No                     No   \n",
       "\n",
       "  change diabetesMed             race payer_code         medical_specialty  \n",
       "0     No          No        Caucasian         MD  Pediatrics-Endocrinology  \n",
       "1     Ch         Yes        Caucasian         MC       Hematology/Oncology  \n",
       "2     No         Yes  AfricanAmerican         HM   ObstetricsandGynecology  \n",
       "3     Ch         Yes        Caucasian         MC          InternalMedicine  \n",
       "4     Ch         Yes        Caucasian         MD       Hematology/Oncology  "
      ]
     },
     "execution_count": 35,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.select_dtypes(include = 'object').head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "id": "7446ac50",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>admission_type_id</th>\n",
       "      <th>discharge_disposition_id</th>\n",
       "      <th>admission_source_id</th>\n",
       "      <th>time_in_hospital</th>\n",
       "      <th>num_lab_procedures</th>\n",
       "      <th>num_procedures</th>\n",
       "      <th>num_medications</th>\n",
       "      <th>number_outpatient</th>\n",
       "      <th>number_emergency</th>\n",
       "      <th>number_inpatient</th>\n",
       "      <th>number_diagnoses</th>\n",
       "      <th>Target</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>6</td>\n",
       "      <td>25</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>41</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>7</td>\n",
       "      <td>3</td>\n",
       "      <td>59</td>\n",
       "      <td>0</td>\n",
       "      <td>18</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>9</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>7</td>\n",
       "      <td>2</td>\n",
       "      <td>11</td>\n",
       "      <td>5</td>\n",
       "      <td>13</td>\n",
       "      <td>2</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>6</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>7</td>\n",
       "      <td>2</td>\n",
       "      <td>44</td>\n",
       "      <td>1</td>\n",
       "      <td>16</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>7</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>7</td>\n",
       "      <td>1</td>\n",
       "      <td>51</td>\n",
       "      <td>0</td>\n",
       "      <td>8</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>5</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   admission_type_id  discharge_disposition_id  admission_source_id  \\\n",
       "0                  6                        25                    1   \n",
       "1                  1                         1                    7   \n",
       "2                  1                         1                    7   \n",
       "3                  1                         1                    7   \n",
       "4                  1                         1                    7   \n",
       "\n",
       "   time_in_hospital  num_lab_procedures  num_procedures  num_medications  \\\n",
       "0                 1                  41               0                1   \n",
       "1                 3                  59               0               18   \n",
       "2                 2                  11               5               13   \n",
       "3                 2                  44               1               16   \n",
       "4                 1                  51               0                8   \n",
       "\n",
       "   number_outpatient  number_emergency  number_inpatient  number_diagnoses  \\\n",
       "0                  0                 0                 0                 1   \n",
       "1                  0                 0                 0                 9   \n",
       "2                  2                 0                 1                 6   \n",
       "3                  0                 0                 0                 7   \n",
       "4                  0                 0                 0                 5   \n",
       "\n",
       "   Target  \n",
       "0       0  \n",
       "1       0  \n",
       "2       0  \n",
       "3       0  \n",
       "4       0  "
      ]
     },
     "execution_count": 36,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.select_dtypes(include = 'number').head()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "762dca59",
   "metadata": {},
   "source": [
    "Inferences:\n",
    "    \n",
    "    -->Number of Catagorical Columns : 35\n",
    "    \n",
    "    -->Number of Numerical Columns : 12 of which one is our target variable"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "35ca0016",
   "metadata": {},
   "source": [
    "### b) Variable categorization (count of numeric and categorical)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "id": "ffe7b87a",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Index(['gender', 'age', 'max_glu_serum', 'A1Cresult', 'metformin',\n",
       "       'repaglinide', 'nateglinide', 'chlorpropamide', 'glimepiride',\n",
       "       'acetohexamide', 'glipizide', 'glyburide', 'tolbutamide',\n",
       "       'pioglitazone', 'rosiglitazone', 'acarbose', 'miglitol', 'troglitazone',\n",
       "       'tolazamide', 'examide', 'citoglipton', 'insulin',\n",
       "       'glyburide-metformin', 'glipizide-metformin',\n",
       "       'glimepiride-pioglitazone', 'metformin-rosiglitazone',\n",
       "       'metformin-pioglitazone', 'change', 'diabetesMed', 'race', 'payer_code',\n",
       "       'medical_specialty', 'diag_1', 'diag_2', 'diag_3'],\n",
       "      dtype='object')"
      ]
     },
     "execution_count": 73,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.select_dtypes(exclude = 'number').columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 75,
   "id": "2792a3fb",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Index(['admission_type_id', 'discharge_disposition_id', 'admission_source_id',\n",
       "       'time_in_hospital', 'num_lab_procedures', 'num_procedures',\n",
       "       'num_medications', 'number_outpatient', 'number_emergency',\n",
       "       'number_inpatient', 'number_diagnoses', 'Target'],\n",
       "      dtype='object')"
      ]
     },
     "execution_count": 75,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.select_dtypes(include = 'number').columns"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a28bd0f1",
   "metadata": {},
   "source": [
    "## I ---> Numerical variables\n",
    "\n",
    "### There are 12 numerical variables and listed below:\n",
    "\n",
    "### 1 ) admission_type_id\n",
    "### 2 ) discharge_disposition_id\n",
    "### 3 ) admission_source_id\n",
    "### 4 ) time_in_hospital\n",
    "### 5 ) num_lab_procedures\n",
    "### 6 ) num_procedures\n",
    "### 7 ) num_medications\n",
    "### 8 ) number_outpatient\n",
    "### 9 ) number_emergency\n",
    "### 10 ) number_inpatient\n",
    "### 11 ) number_diagnoses\n",
    "### 12 ) Target\n",
    "       "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a2e2601d",
   "metadata": {},
   "source": [
    "## II ---> Categorical variables\n",
    "\n",
    "### There are 35 categorical variables and listed below:\n",
    "\n",
    "### 1 ) gender\n",
    "### 2 ) age\n",
    "### 3 ) max_glu_serum\n",
    "### 4 ) A1Cresult\n",
    "### 5 ) metformin\n",
    "### 6 ) repaglinide\n",
    "### 7 ) nateglinide\n",
    "### 8 ) chlorpropamide\n",
    "### 9 ) glimepiride\n",
    "### 10 ) acetohexamide\n",
    "### 11 ) glipizide\n",
    "### 12 ) glyburide\n",
    "### 13 ) tolbutamide\n",
    "### 14 ) pioglitazone\n",
    "### 15 ) rosiglitazone\n",
    "### 16 ) acarbose\n",
    "### 17 ) miglitol\n",
    "### 18 ) troglitazone\n",
    "### 19 ) tolazamide\n",
    "### 20 ) examide\n",
    "### 21 ) citoglipton\n",
    "### 22 ) insulin\n",
    "### 23 ) glyburide-metformin\n",
    "### 24 ) glipizide-metformin\n",
    "### 25 ) glimepiride-pioglitazone\n",
    "### 26 ) metformin-rosiglitazone\n",
    "### 27 ) metformin-pioglitazone\n",
    "### 28 ) change\n",
    "### 29 ) diabetesMed\n",
    "### 30 ) race\n",
    "### 31 ) payer_code \n",
    "### 32 ) medical_specialty\n",
    "### 33 ) diag_1\n",
    "### 34 ) diag_2\n",
    "### 35 ) diag_3\n",
    "\n",
    "       "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "65bd0bef",
   "metadata": {},
   "source": [
    "#### Pre Processing Data Analysis (count of missing/ null values, redundant columns, etc.)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4f52034e",
   "metadata": {},
   "source": [
    "<b>Null or Missing Values</b>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 79,
   "id": "80b11cd2",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Your selected dataframe has 47 columns and 101766 Rows.\n",
      "There are 0 columns that have missing values.\n"
     ]
    }
   ],
   "source": [
    "NA_df =  missing_zero_values_table(df)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a7aa7077",
   "metadata": {},
   "source": [
    "### II) Redundant Columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 82,
   "id": "cc807d7f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Column Name: gender\n",
      "Number of Unique Variables in gender is:  3\n",
      "Column Name: age\n",
      "Number of Unique Variables in age is:  10\n",
      "Column Name: admission_type_id\n",
      "Number of Unique Variables in admission_type_id is:  8\n",
      "Column Name: discharge_disposition_id\n",
      "Number of Unique Variables in discharge_disposition_id is:  26\n",
      "Column Name: admission_source_id\n",
      "Number of Unique Variables in admission_source_id is:  17\n",
      "Column Name: time_in_hospital\n",
      "Number of Unique Variables in time_in_hospital is:  14\n",
      "Column Name: num_lab_procedures\n",
      "Number of Unique Variables in num_lab_procedures is:  118\n",
      "Column Name: num_procedures\n",
      "Number of Unique Variables in num_procedures is:  7\n",
      "Column Name: num_medications\n",
      "Number of Unique Variables in num_medications is:  75\n",
      "Column Name: number_outpatient\n",
      "Number of Unique Variables in number_outpatient is:  39\n",
      "Column Name: number_emergency\n",
      "Number of Unique Variables in number_emergency is:  33\n",
      "Column Name: number_inpatient\n",
      "Number of Unique Variables in number_inpatient is:  21\n",
      "Column Name: number_diagnoses\n",
      "Number of Unique Variables in number_diagnoses is:  16\n",
      "Column Name: max_glu_serum\n",
      "Number of Unique Variables in max_glu_serum is:  4\n",
      "Column Name: A1Cresult\n",
      "Number of Unique Variables in A1Cresult is:  4\n",
      "Column Name: metformin\n",
      "Number of Unique Variables in metformin is:  4\n",
      "Column Name: repaglinide\n",
      "Number of Unique Variables in repaglinide is:  4\n",
      "Column Name: nateglinide\n",
      "Number of Unique Variables in nateglinide is:  4\n",
      "Column Name: chlorpropamide\n",
      "Number of Unique Variables in chlorpropamide is:  4\n",
      "Column Name: glimepiride\n",
      "Number of Unique Variables in glimepiride is:  4\n",
      "Column Name: acetohexamide\n",
      "Number of Unique Variables in acetohexamide is:  2\n",
      "Column Name: glipizide\n",
      "Number of Unique Variables in glipizide is:  4\n",
      "Column Name: glyburide\n",
      "Number of Unique Variables in glyburide is:  4\n",
      "Column Name: tolbutamide\n",
      "Number of Unique Variables in tolbutamide is:  2\n",
      "Column Name: pioglitazone\n",
      "Number of Unique Variables in pioglitazone is:  4\n",
      "Column Name: rosiglitazone\n",
      "Number of Unique Variables in rosiglitazone is:  4\n",
      "Column Name: acarbose\n",
      "Number of Unique Variables in acarbose is:  4\n",
      "Column Name: miglitol\n",
      "Number of Unique Variables in miglitol is:  4\n",
      "Column Name: troglitazone\n",
      "Number of Unique Variables in troglitazone is:  2\n",
      "Column Name: tolazamide\n",
      "Number of Unique Variables in tolazamide is:  3\n",
      "Column Name: examide\n",
      "Number of Unique Variables in examide is:  1\n",
      "Column Name: citoglipton\n",
      "Number of Unique Variables in citoglipton is:  1\n",
      "Column Name: insulin\n",
      "Number of Unique Variables in insulin is:  4\n",
      "Column Name: glyburide-metformin\n",
      "Number of Unique Variables in glyburide-metformin is:  4\n",
      "Column Name: glipizide-metformin\n",
      "Number of Unique Variables in glipizide-metformin is:  2\n",
      "Column Name: glimepiride-pioglitazone\n",
      "Number of Unique Variables in glimepiride-pioglitazone is:  2\n",
      "Column Name: metformin-rosiglitazone\n",
      "Number of Unique Variables in metformin-rosiglitazone is:  2\n",
      "Column Name: metformin-pioglitazone\n",
      "Number of Unique Variables in metformin-pioglitazone is:  2\n",
      "Column Name: change\n",
      "Number of Unique Variables in change is:  2\n",
      "Column Name: diabetesMed\n",
      "Number of Unique Variables in diabetesMed is:  2\n",
      "Column Name: Target\n",
      "Number of Unique Variables in Target is:  2\n",
      "Column Name: race\n",
      "Number of Unique Variables in race is:  5\n",
      "Column Name: payer_code\n",
      "Number of Unique Variables in payer_code is:  17\n",
      "Column Name: medical_specialty\n",
      "Number of Unique Variables in medical_specialty is:  75\n",
      "Column Name: diag_1\n",
      "Number of Unique Variables in diag_1 is:  716\n",
      "Column Name: diag_2\n",
      "Number of Unique Variables in diag_2 is:  748\n",
      "Column Name: diag_3\n",
      "Number of Unique Variables in diag_3 is:  789\n"
     ]
    }
   ],
   "source": [
    "k = df.columns\n",
    "for i in k:\n",
    "    print('Column Name:',i)\n",
    "    print('Number of Unique Variables in',i,'is: ',df[i].nunique())"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c231c011",
   "metadata": {},
   "source": [
    "As obvious, RowNumber and CustomerId are unique variables to every entry and do not assist our model in any way since they have 10,000 unique values. \n",
    "Similarly, Surname has 2932 unique values and it does not assist our model in any way.\n",
    "\n",
    "### Redundant columns are RowNumber, CustomerId and Surname\n",
    "\n",
    "### We must therefore drop them as they are redundant variables.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "133572ce",
   "metadata": {},
   "outputs": [],
   "source": [
    "drop_cols = ['RowNumber','CustomerId','Surname']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "782aab9e",
   "metadata": {},
   "outputs": [],
   "source": [
    "df.drop(columns = drop_cols, axis = 1, inplace = True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d32ad8ae",
   "metadata": {},
   "outputs": [],
   "source": [
    "cat_cols = df.select_dtypes(include = 'object').columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "18012588",
   "metadata": {},
   "outputs": [],
   "source": [
    "cat_cols"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b67fddb3",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_numeric           =   df.select_dtypes(include = 'number')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1238d6a6",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_numeric.columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "755698e2",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_numeric_    =  pd.DataFrame(df_numeric)\n",
    "type(df_numeric_)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "577bb7d4",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_categorical   =   df.select_dtypes(include = 'object')\n",
    "df_categorical.columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8a0f3793",
   "metadata": {},
   "outputs": [],
   "source": [
    "type(df_categorical)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5dad5677",
   "metadata": {},
   "source": [
    "#### Observations\n",
    "\n",
    "We have now dropped our redundant column and our data is ready for further processing."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f451602f",
   "metadata": {},
   "source": [
    "### d) Alternate sources of data that can supplement the core dataset (at least 2-3 columns)\n",
    "1) Price Perception\n",
    "2) Trust"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "084b53eb",
   "metadata": {},
   "source": [
    "### e) Project Justification -  Project Statement, Complexity involved, Project Outcome Commercial, Academic or Social value\n",
    "\n",
    "#### Project Statement\n",
    "\n",
    "To develop a robust customer churn prediction model for the European MNC bank operating in France, Germany, and Spain. \n",
    "\n",
    "#### Complexity involved - Low\n",
    "\n",
    "#### Project Outcome Commercial, Academic or Social value - Commercial"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5c0faff1",
   "metadata": {},
   "source": [
    "# 3) Data Exploration (EDA)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0fd5f752",
   "metadata": {},
   "source": [
    "### Converting and Labelling the Data\n",
    "\n",
    "Since the majority of machine learning algorithms are created to operate with numerical data, categorical data is converted to numerical data using label encoding techniques. \n",
    "\n",
    "**Label Encoding** is a common technique for converting categorical variables into numerical values. Each unique category value is assigned a unique integer based on alphabetical or numerical ordering."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5e1a50f7",
   "metadata": {},
   "outputs": [],
   "source": [
    "df.select_dtypes(include = 'object').columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "95a5dfd4",
   "metadata": {},
   "outputs": [],
   "source": [
    "new_df            =   df.copy()\n",
    "new_df['Exited']  =   df['Exited']"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a21867d4",
   "metadata": {},
   "source": [
    "Variables ('Geography', 'Gender', 'Card Type') need to be encoded for further analysis and preprocessing."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ce36598b",
   "metadata": {},
   "outputs": [],
   "source": [
    "cat_newdf         =  new_df.select_dtypes(exclude=['int64','float64'])\n",
    "cat_cols          =  cat_newdf.columns\n",
    "for col in cat_cols:\n",
    "    label_enc(new_df, col)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0061fd2a",
   "metadata": {},
   "outputs": [],
   "source": [
    "new_df.columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3447cb8c",
   "metadata": {},
   "outputs": [],
   "source": [
    "new_df.head().T"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "892ef817",
   "metadata": {},
   "source": [
    "### 3.1) Relationship between variables"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4e44bf64",
   "metadata": {},
   "source": [
    "### Numerical variable vs Target variable"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6d222211",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define the numerical and categorical columns for analysis\n",
    "numerical_columns    = ['CreditScore', 'Age', 'Tenure', 'Balance', 'NumOfProducts', 'HasCrCard','IsActiveMember',\\\n",
    "                       'EstimatedSalary', 'Complain', 'Satisfaction Score', 'Point Earned']\n",
    "categorical_columns = ['Exited']\n",
    "\n",
    "for  var in numerical_columns :\n",
    "     new_df.boxplot(var, by = 'Exited')\n",
    "     plt.show()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "330f9600",
   "metadata": {},
   "source": [
    "### 3.2) Check for \n",
    "### 3.2.1) multi-collinearity"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5197f6a3",
   "metadata": {},
   "outputs": [],
   "source": [
    "X = new_df[['CreditScore', 'Age', 'Tenure', 'Balance','NumOfProducts', 'HasCrCard', 'IsActiveMember', 'EstimatedSalary',\n",
    "            'Complain', 'Satisfaction Score', 'Point Earned', 'Geography_code', 'Gender_code', 'Card Type_code']]\n",
    "Y = new_df['Exited']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "85f1f333",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Checking for multicolleniarty in our independant variables\n",
    "# VIF dataframe\n",
    "vif_data = pd.DataFrame()\n",
    "vif_data[\"feature\"] = X.columns\n",
    "  \n",
    "# calculating VIF for each feature\n",
    "vif_data[\"VIF\"] = [variance_inflation_factor(X.values, i) for i in range(len(X.columns))]\n",
    "\n",
    "vif_data"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "09647245",
   "metadata": {},
   "source": [
    "As visible, the following five (5) variables are having high **VIF** value greater than 5.\n",
    "\n",
    "### Interpretation of VIF value\n",
    "\n",
    "The higher the value, the greater the correlation of the variable with other variables. Values of more than 4 or 5 are sometimes regarded as being moderate to high, with values of 10 or more being regarded as very high.\n",
    "\n",
    "1) CreditScore, 2) Age, 3) NumOfProducts, 4) Satisfaction Score and 5) Point Earned.\t\n",
    "\n",
    "We need to remove variables having greater than the threshold value of 5 from our dataset."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "19038ea7",
   "metadata": {},
   "source": [
    "### 3.2.2) Distribution of variables"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0231fba1",
   "metadata": {},
   "source": [
    "## a) Histogram for continuous numerical variable\n",
    "\n",
    "### A histogram helps to understand the distribution of values in single continuous column"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1a99fbc4",
   "metadata": {},
   "outputs": [],
   "source": [
    "num_vars             =    ['Age',  'Balance',  'EstimatedSalary']\n",
    "for  var in num_vars:\n",
    "     hist_numerical(var, df)    "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6fe4b2e5",
   "metadata": {},
   "source": [
    "### Observations\n",
    "\n",
    "**None of the continuous numerical variables ('Age',  'Balance',  'EstimatedSalary') are normally distributed.**"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "322484c6",
   "metadata": {},
   "source": [
    "## b) Bar charts for categorical variables\n",
    "\n",
    "### A bar chart (aka bar graph, column chart) plots numeric values for levels of a categorical feature as bars."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fa164f33",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define the categorical columns for analysis\n",
    "categorical_columns    =   ['Geography', 'Gender', 'Card Type']\n",
    "i                      =   1\n",
    "for var in categorical_columns :\n",
    "    draw_countplot(new_df, i, var)\n",
    "    i += 1"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "84c9ea9d",
   "metadata": {},
   "source": [
    "### Box charts for numerical variables grouped by categorical variables"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "749bb647",
   "metadata": {},
   "outputs": [],
   "source": [
    "new_df.columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1ce88535",
   "metadata": {},
   "outputs": [],
   "source": [
    "numerical_columns    = ['CreditScore', 'Age', 'Tenure', 'Balance', 'EstimatedSalary', 'Point Earned']\n",
    "i  =  1\n",
    "###\n",
    "for var in numerical_columns :\n",
    "    draw_boxplot2(new_df, i, var)\n",
    "    i += 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "db88f857",
   "metadata": {},
   "outputs": [],
   "source": [
    "### \n",
    "i  =  1\n",
    "for var in numerical_columns :\n",
    "    draw_boxplot2(new_df, i, var)\n",
    "    i += 1\n",
    "###"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dbc0905f",
   "metadata": {},
   "outputs": [],
   "source": [
    "titlex  =  '\\n\\nBoxplot for age \\n'\n",
    "sns.catplot(data = df, x = 'Complain', y = 'Age', hue = 'Exited', kind = 'box')\n",
    "plt.xlabel('Complain')\n",
    "plt.ylabel('Age')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bc81d164",
   "metadata": {},
   "source": [
    "### 3.2.3) Presence of outliers and its treatment \n",
    "\n",
    "Outliers badly affect mean and standard deviation of the dataset.  It increases the error variance and reduces the power of statistical tests. By applying outlier treatment, machine learning practitioners can handle extreme values effectively. The primary goals of outlier treatment are: Identifying Outliers: Through various statistical methods, such as visualizations and mathematical approaches, outliers can be detected within a dataset.\n",
    "\n",
    "We are interested to identify the outliers in our continuos numerical variables such as 'Age',  'Balance',  'EstimatedSalary' that affects the mean & standard deviation rather than the discrete numerical variables. Discrete variables are typically categorical, meaning they take on a limited number of values or categories. \n",
    "\n",
    "### However, if the outlier is physically possible you should consider it."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "93444d6a",
   "metadata": {},
   "outputs": [],
   "source": [
    "for  var in num_vars:\n",
    "     identify_outliers(df, var)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "65a51ff4",
   "metadata": {},
   "source": [
    "### Ob servations\n",
    "\n",
    "We have checked the presence of outliers in the continuous numerical variables ('Age', 'Balance', 'EstimatedSalary').\n",
    "Only the variable, Age is having the outlier. We observe that the maximum value is 92 and it is a valid & phisically possible value. **Hence, we will retain the outliers for the variable, Age**. "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2f3d1d75",
   "metadata": {},
   "source": [
    "### 3.2.4) Statistical significance of variables"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4ad2a08c",
   "metadata": {},
   "source": [
    "a) Numerical variables"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "24285b23",
   "metadata": {},
   "source": [
    "An unpaired t-test (also known as an independent t-test) is a statistical procedure that compares the averages/means of two independent or unrelated group of numerical variables to determine if there is a significant difference between the two.\n",
    "\n",
    "Hypotheses are assumptions about reality whose validity is possible but not yet proven. Two hypotheses are always formulated that assert exactly the opposite. These two hypotheses are the null hypothesis and the alternative hypothesis.\n",
    "\n",
    "Null hypothesis $H_0$\tAlternative hypothesis $H_1$\n",
    "\n",
    "There is no mean difference between the two groups in the population.\n",
    "\n",
    " Two population means are equal.\n",
    " The two groups are from the same population.\n",
    " $H_0$: 1 = 2\n",
    "\n",
    " Example: There is no difference between each variable of those who churned and who not churned.\n",
    "\n",
    "There is a mean difference between the two groups in the population.\n",
    "\n",
    " The two population means are not equal.\n",
    " The two groups are not from the same population.\n",
    "  $H_1$: 1 2\n",
    "\n",
    "There is mean difference between the two groups in the population."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a6863539",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define the numerical and categorical columns for analysis\n",
    "numerical_columns    = ['CreditScore', 'Age', 'Tenure', 'Balance', 'NumOfProducts', 'HasCrCard','IsActiveMember',\\\n",
    "                       'EstimatedSalary', 'Complain', 'Satisfaction Score', 'Point Earned']\n",
    "for var in numerical_columns :\n",
    "    chk_ttest(var)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d7c7ef0e",
   "metadata": {},
   "source": [
    "### Observations\n",
    "\n",
    "Based on the unpaired t test, we find that\n",
    "* there is **a statistically significant difference** between the mean values of two groups of the Variable, listed below:\n",
    "1) CreditScore \n",
    "2) Age\n",
    "3) Balance \n",
    "4) NumOfProducts \n",
    "5) IsActiveMember \n",
    "6) Complain \n",
    "\n",
    "* there is **no statistically significant difference** between the mean values of two groups of the Variable, listed below:\n",
    "1) Tenure\n",
    "2) HasCrCard\n",
    "3) EstimatedSalary\n",
    "4) Satisfaction Score\n",
    "5) Point Earned"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ea1d26a5",
   "metadata": {},
   "source": [
    "### Shapiro-Wilk test is a test of normality, it determines whether the given sample comes from the normal distribution or not. "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c45e69d5",
   "metadata": {},
   "source": [
    "Null hypothesis: $H_0$: Samples are drawn from normal distribution.\n",
    "Alternative hypothesis: $H_1$: Samples are NOT drawn from normal distribution.        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "93a1b0b7",
   "metadata": {},
   "outputs": [],
   "source": [
    "num_vars             =    ['Age',  'Balance',  'EstimatedSalary']\n",
    "i                    =    1\n",
    "for  var in num_vars:\n",
    "     x          =   df[var]\n",
    "     stat, pval =   stats.shapiro(x)\n",
    "     print(\"\\n\\n{0}: P value of Shapiro test for {1} is {2}\".format(i, var, pval))\n",
    "     i         +=  1\n",
    "     if pval < 0.05:\n",
    "        print(\"\\nSamples of {} are NOT from a normal distribution\".format(var))    \n",
    "     else:\n",
    "        print(\"\\nSamples of {} are from a normal distribution\".format(var))        \n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e0ff773c",
   "metadata": {},
   "source": [
    "### Observations\n",
    "\n",
    "Based on the Shapiro test for normality, we observe the following:\n",
    "'Age',  'Balance',  'EstimatedSalary' are **not normally** dustributed."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9992aad6",
   "metadata": {},
   "source": [
    "b) Categorical variables"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b0a1caab",
   "metadata": {},
   "source": [
    "The $\\chi^2$ - (Chi Sqaure) test of independence analysis utilizes a cross tabulation table between the variables of interest r rows and c columns. \n",
    "\n",
    "Based on the cell counts, it is possible to test if there is a relationship, dependence, between the variables and to estimate the strength of the relationship. \n",
    "\n",
    "#### Assumptions\n",
    "\n",
    "* The two samples are independent\n",
    "* No expected cell count is = 0\n",
    "* No more than 20% of the cells have and expected cell count < 5\n",
    "\n",
    "### Hypothesis\n",
    "\n",
    "Null hypothesis $H_0$: Variables are independent\t\n",
    "\n",
    "Alternative hypothesis $H_1$: Variables are NOT independent\t"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9c329d85",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define the categorical columns for analysis\n",
    "categorical_columns    =   ['Geography', 'Gender', 'Card Type']\n",
    "i  =  1\n",
    "for var in categorical_columns :\n",
    "    chk_chisq(i, var)\n",
    "    i += 1"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "60c97c3f",
   "metadata": {},
   "source": [
    "### Observations\n",
    "\n",
    "We have checked each categorical independent variable with our target categorical variable using Chi Square test of independence. We observed the assumption for chi-square test of independence *(No more than 20% of the cells have and expected cell count < 5)* is satisfied.\n",
    "\n",
    "**Each of the independent variables, 'Geography', 'Gender' and the target variable, Exited are dependent.**\n",
    "\n",
    "**Independent variable, 'Card Type' and the target variable, Exited are independent.**"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d3c86e74",
   "metadata": {},
   "source": [
    "###  Calculate Correlation Between Continuous & Binary Target Variable\n",
    "\n",
    "Point biserial correlation is used to calculate the correlation between a binary categorical variable (a variable that can only take on two values) and a continuous variable and has the following properties:\n",
    "\n",
    "Point biserial correlation can range between -1 and 1.\n",
    "For each group created by the binary variable, it is assumed that the continuous variable is normally distributed with equal variances.\n",
    "For each group created by the binary variable, it is assumed that there are no extreme outliers.\n",
    "\n",
    "The hypotheses for point biserial correlation thus result in:\n",
    "\n",
    "Null hypothesis: The correlation coefficient r = 0 (There is no correlation)\n",
    "\n",
    "Alternative hypothesis: The correlation coefficient r  0 (There is a correlation)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "05ab8ef8",
   "metadata": {},
   "outputs": [],
   "source": [
    "Target            =  df['Exited']\n",
    "colsn             =  ['Age',  'Balance',  'EstimatedSalary']\n",
    "for  col1 in colsn:\n",
    "    x   =   df[col1]\n",
    "    y   =   df['Exited']\n",
    "    pointbiserialr(df, col1, y)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "391746c7",
   "metadata": {},
   "source": [
    "### 3.2.5) class imbalance and its treatment"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "38aa1ae9",
   "metadata": {},
   "outputs": [],
   "source": [
    "b = Y.value_counts()\n",
    "sns.barplot(x = b.index,y = b.values)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a6b67756",
   "metadata": {},
   "source": [
    "### Observations"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a472cb37",
   "metadata": {},
   "source": [
    "As visible, our data is highly imbalanced. Imbalanced datasets can lead to a bias towards the majority class, as the model is trained on a majority of samples from the majority class. This can result in poor performance in the minority class.\n",
    "Hence, we need to treat data imbalance.\n",
    "\n",
    "One approach to addressing imbalanced datasets is to oversample the minority class. The simplest approach involves duplicating examples in the minority class, although these examples dont add any new information to the model. Instead, new examples can be synthesized from the existing examples. This is a type of data augmentation for the minority class and is referred to as the Synthetic Minority Oversampling Technique, or **SMOTE** for short.\n",
    "\n",
    "**Another method is under-sampling.**\n",
    "\n",
    "Under-sampling balances the dataset by reducing the size of the abundant class. This method is used when quantity of data is sufficient. By keeping all samples in the rare class and randomly selecting an equal number of samples in the abundant class, a balanced new dataset can be retrieved for further modelling.\n",
    "\n",
    "We prefer Over-sampling by SMOTE method."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "eec33a7f",
   "metadata": {},
   "source": [
    "# 4) Feature Engineering"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b5c82ce0",
   "metadata": {},
   "source": [
    "## 4.1) Whether any transformations required\n",
    "\n",
    "Data transformation is used when data needs to be converted to match that of the destination system."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3b44a1f0",
   "metadata": {},
   "source": [
    "**We have performed label encoding to make our data suitable for model building.**"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "273dd375",
   "metadata": {},
   "source": [
    "## 4.2) Scaling the data\n",
    "\n",
    "In Data Processing, we try to change the data in such a way that the model can process it without any problems. And Feature Scaling is one such process in which we transform the data into a better version. Feature Scaling is done to normalize the features in the dataset into a finite range."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ec5c6dac",
   "metadata": {},
   "source": [
    "The concept of standardization comes into picture when continuous independent variables are measured at different scales. https://www.listendata.com/2017/04/how-to-standardize-variable-in-regression.html\n",
    "\n",
    "Data scaling is applied to numeric columns. In our dataset we have three continuous numerical columns:.\n",
    "\n",
    "1) Age\n",
    "2) Balance\n",
    "3) EstimatedSalary\n",
    "\n",
    "The script below filters these three columns and removes the remaining columns."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "741b23b2",
   "metadata": {},
   "outputs": [],
   "source": [
    "data_numeric = new_df.filter(['Age',  'Balance',  'EstimatedSalary'], axis = 1)\n",
    "\n",
    "data_numeric.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4dce972b",
   "metadata": {},
   "source": [
    "Lets plot some statistical values for the columns in our dataset using the describe() method."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "042e83a5",
   "metadata": {},
   "outputs": [],
   "source": [
    "data_numeric.describe()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e74b40e8",
   "metadata": {},
   "source": [
    "### Robust scaling \n",
    "\n",
    "Both standard and robust scalers transform inputs to comparable scales. The difference lies in how they scale raw input values.\n",
    "\n",
    "Standard scaling uses mean and standard deviation. Robust scaling uses median and interquartile range (IQR) instead.\n",
    "\n",
    "Robust scaling answers a simple question. How far is each data point from the inputs median? \n",
    "\n",
    "### The fact that robust scaling uses median and IQR makes it resistant to outliers. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bdef8310",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.preprocessing import RobustScaler\n",
    " \n",
    "robust_scaler =   RobustScaler()\n",
    "cols          =   ['Age',  'Balance',  'EstimatedSalary']\n",
    "robust_arr    =   data_numeric.to_numpy()\n",
    "robust_scaler.fit(robust_arr)\n",
    "# scale all data points using median and IQR\n",
    "robust_scaled_data =  robust_scaler.transform(robust_arr)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "47e108b3",
   "metadata": {},
   "outputs": [],
   "source": [
    "robust_df     =   pd.DataFrame(robust_scaled_data, columns = cols)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "56caf609",
   "metadata": {},
   "outputs": [],
   "source": [
    "robust_df.describe()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7d001b23",
   "metadata": {},
   "source": [
    "## 4.3) Feature selection "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "823e6645",
   "metadata": {},
   "source": [
    "## Automatically select the number of features\n",
    "\n",
    "The RFE method is available via the RFE class in scikit-learn.\n",
    "\n",
    "RFE is a transform. To use it, first the class is configured with the chosen algorithm specified via the estimator argument and the number of features to select via the n_features_to_select argument.\n",
    "\n",
    "The algorithm must provide a way to calculate important scores, such as a decision tree. The algorithm used in RFE does not have to be the algorithm that is fit on the selected features; different algorithms can be used.\n",
    "\n",
    "Once configured, the class must be fit on a training dataset to select the features by calling the fit() function. After the class is fit, the choice of input variables can be seen via the support_ attribute that provides a True or False for each input variable.\n",
    "\n",
    "It can then be applied to the dataset by calling the transform() function."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6175b8d4",
   "metadata": {},
   "outputs": [],
   "source": [
    "###\n",
    "### We need to remove the not-standardized 3 columns and add standardised 3 columns\n",
    "###\n",
    "new_df.drop('Age', axis = 1, inplace = True)\n",
    "new_df.drop('Balance', axis = 1, inplace = True)\n",
    "new_df.drop('EstimatedSalary', axis = 1, inplace = True)\n",
    "\n",
    "['Age',  'Balance',  'EstimatedSalary']\n",
    "\n",
    "new_df['Age']             =  robust_df['Age'] \n",
    "new_df['Balance']         =  robust_df['Balance'] \n",
    "new_df['EstimatedSalary'] =  robust_df['EstimatedSalary'] \n",
    "\n",
    "\n",
    "# Adding the target variable\n",
    "new_df['Exited']          =  new_df['Exited']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "949986c2",
   "metadata": {},
   "outputs": [],
   "source": [
    "y             =    new_df['Exited']\n",
    "X             =    new_df[['CreditScore', 'Tenure', 'NumOfProducts','HasCrCard', 'IsActiveMember',\\\n",
    "                            'Complain','Satisfaction Score', 'Point Earned', 'Geography_code',\\\n",
    "                           'Gender_code', 'Card Type_code', 'Age', 'Balance', 'EstimatedSalary']]                       \n",
    "print('X dimension {}'. format(X.shape))\n",
    "print('y dimension {}'. format(y.shape))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d3ca2062",
   "metadata": {},
   "outputs": [],
   "source": [
    "new_df.columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "68aed6f1",
   "metadata": {},
   "outputs": [],
   "source": [
    "# create pipeline\n",
    "rfe                    =     RFE(estimator = RandomForestClassifier(), n_features_to_select = 10)\n",
    "model                  =     RandomForestClassifier(random_state = 42)\n",
    "pipeline               =     Pipeline(steps=[('s',rfe),('m',model)])\n",
    "# evaluate model\n",
    "cv                     =    RepeatedStratifiedKFold(n_splits = 10, n_repeats = 3, random_state = 1)\n",
    "n_scores               =    cross_val_score(pipeline, X, y, scoring = 'balanced_accuracy', cv = cv, n_jobs = -1, error_score = 'raise')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7bc0a000",
   "metadata": {},
   "outputs": [],
   "source": [
    "# report performance\n",
    "print('balanced_accuracy: %.3f (%.3f)' % (np.mean(n_scores), np.std(n_scores)))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2bbb4a95",
   "metadata": {},
   "source": [
    "### Observation\n",
    "\n",
    "we can see the RFE that uses a Random Forest and selects 10 features and then fits a model on the selected features achieves a balanced accuracy of about 100 %. \n",
    "\n",
    "**Balanced accuracy** in binary and multiclass classification problems to deal with imbalanced datasets. It is defined as the average of recall obtained on each class."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7d34a6df",
   "metadata": {},
   "source": [
    "Fit an RFE model on the whole dataset and selects five features, then reports each feature column index (0 to 9), whether it was selected or not (True or False), and the relative feature ranking.\n",
    "\n",
    "The support_ attribute reports true or false as to which features in order of column index were included and the ranking_ attribute reports the relative ranking of features in the same order."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "eeac42a1",
   "metadata": {},
   "outputs": [],
   "source": [
    "# fit RFE\n",
    "rfe.fit(X, y)\n",
    "# summarize all features\n",
    "for i in range(X.shape[1]):\n",
    " print('Column: %d, Selected %s, Rank: %.3f' % (i, rfe.support_[i], rfe.ranking_[i]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b2d3a3cc",
   "metadata": {},
   "outputs": [],
   "source": [
    "x_names   =   X.columns\n",
    "for i in range(X.shape[1]):\n",
    "     namex    =  x_names[i]\n",
    "     print('Column: %d, Name: %s Selected %s, Rank: %.3f' % (i, namex, rfe.support_[i], rfe.ranking_[i]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ec806b27",
   "metadata": {},
   "outputs": [],
   "source": [
    "selected_X  =   []"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b1d72197",
   "metadata": {},
   "outputs": [],
   "source": [
    "for i in range(X.shape[1]):\n",
    "     namex    =  x_names[i]\n",
    "     if (rfe.support_[i] == True):\n",
    "         selected_X.append(namex)\n",
    "         print('Column: %d, Name: %s Selected %s, Rank: %.3f' % (i, namex, rfe.support_[i], rfe.ranking_[i]))\n",
    "print(\"\\nSelected important features {}\".format(selected_X))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cbbf3ebe",
   "metadata": {},
   "outputs": [],
   "source": [
    "selected_X = ['CreditScore', 'Tenure', 'NumOfProducts', 'IsActiveMember', 'Complain', 'Point Earned', 'Geography_code', 'Age', 'Balance', 'EstimatedSalary']"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dbeaa1c2",
   "metadata": {},
   "source": [
    "### Observation\n",
    "\n",
    "We have selected 10 features which are important in predicting the target variable.\n",
    "\n",
    "**We shall use this data for our model building.**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "511515df",
   "metadata": {},
   "outputs": [],
   "source": [
    "X_   =    X.loc[:, selected_X]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "61668ff8",
   "metadata": {},
   "outputs": [],
   "source": [
    "X_.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e1640f96",
   "metadata": {},
   "outputs": [],
   "source": [
    "time_elapsed_secs     = time.time() - start_time\n",
    "\n",
    "time_elapsed_msg = \"Execution took: %s secs (Wall clock time)\" % timedelta(seconds=round(time_elapsed_secs))\n",
    "\n",
    "print(time_elapsed_msg)   "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "79fbd670",
   "metadata": {},
   "source": [
    "## 4.4) Dimensionality reduction\n",
    "\n",
    "Since we have selected top 15 variables affecting the dependent variable, our dataset is Not Huge. We are not going to apply dimensionality reduction such as Principal Component Analysis or Factor Analysis etc.\n",
    "\n",
    "We use PCA when you have high-dimensional data to reduce its dimensionality while preserving most of the variance, simplifying analysis and visualization."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7b46e0ac",
   "metadata": {},
   "source": [
    "# 5) Assumptions\n",
    "\n",
    "## 5.1) Check for the assumptions to be satisfied for each of the models in \n",
    "o\tClassification  Decision Tree, Random Forest, SVM, Bagged and boosted models"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "79767502",
   "metadata": {},
   "source": [
    "### Base model checking"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "21dc0147",
   "metadata": {},
   "outputs": [],
   "source": [
    "X_.isnull().sum()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c8270cab",
   "metadata": {},
   "outputs": [],
   "source": [
    "n_splits     = 10\n",
    "random_state = 123456\n",
    "\n",
    "compare_models(n_splits, random_state, X_, y)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8692be96",
   "metadata": {},
   "source": [
    "## Assumptions Check for Logistic Regression\n",
    "\n",
    "### Assumption 1 - Binary logistic regression requires the target / dependent variable to be binary. \n",
    "\n",
    "For a binary regression, the factor level 1 of the dependent variable should represent the desired outcome (such as Success etc..).\n",
    "\n",
    "### Assumption 2 -  Only the meaningful variables should be included.\n",
    "\n",
    "### Assumption 3 -The predictor variables should not be correlated to each other meaning the model should have little or no multicollinearity.\n",
    "\n",
    "### Assumption 4 - The independent variables are linearly related to the log odds.\n",
    "\n",
    "### Assumption 5 -  Logistic regression requires quite a large number of observations."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a6a2434a",
   "metadata": {},
   "source": [
    "#### Assumption 1 - Binary logistic regression requires the target / dependent variable to be binary. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5e739e5f",
   "metadata": {},
   "outputs": [],
   "source": [
    "new_df[\"Exited\"].value_counts()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a61ba567",
   "metadata": {},
   "source": [
    "### Observations\n",
    "\n",
    "The target variable is categorical having 0 and 1 binary"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ec1558f5",
   "metadata": {},
   "source": [
    "#### Assumption 2 -  Only the meaningful variables should be included."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "78470958",
   "metadata": {},
   "source": [
    "We have ensured that there are no unwanted variables selected for model building."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6a1f38f8",
   "metadata": {},
   "source": [
    "#### Assumption 3 -The predictor variables should not be correlated to each other meaning the model should have little or no multicollinearity."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "696d67d6",
   "metadata": {},
   "outputs": [],
   "source": [
    "def show_VIF(X):\n",
    "    vif = [variance_inflation_factor(X.values, i) for i in range(X.shape[1])]\n",
    "\n",
    "    vif_df = pd.DataFrame({'Feature': X.columns, 'VIF': vif})\n",
    "    vif_df.sort_values(by = ['VIF'], ascending = False, inplace = True)\n",
    "\n",
    "    return(vif_df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ab9e613c",
   "metadata": {},
   "outputs": [],
   "source": [
    "show_VIF(X_)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0644f5fd",
   "metadata": {},
   "source": [
    "### Observations\n",
    "\n",
    "The following variables are highly colliear as their VIF values exceed the threshold value of 5:\n",
    "* 1)  CreditScore\t(14.057075)\n",
    "* 2)  NumOfProducts\t(7.833422)\n",
    "* 3)  PointsEarned\t(7.243992)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8c715b89",
   "metadata": {},
   "outputs": [],
   "source": [
    "calculate_vif_(X_, thresh = 5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b40856ee",
   "metadata": {},
   "outputs": [],
   "source": [
    "X_collinear = X_.loc[:, ['Tenure', 'IsActiveMember', 'Complain', 'Point Earned','Geography_code', 'Age', 'Balance', 'EstimatedSalary']]\n",
    "\n",
    "show_VIF(X_collinear)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c3177a48",
   "metadata": {},
   "source": [
    "### Observations\n",
    "\n",
    "The following variables are non-collinear:\n",
    "1) Point Earned\n",
    "2) Tenure\n",
    "3) IsActiveMember\n",
    "4) Geography_code\t\n",
    "5) Complain\n",
    "6) Age\n",
    "7) Balance\n",
    "8) EstimatedSalary"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d84e1ff7",
   "metadata": {},
   "source": [
    "#### Assumption 4 - The independent variables are linearly related to the log odds."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9603538c",
   "metadata": {},
   "source": [
    "We need to check the assumption of Independent variables are linearly related to the log odds.\n",
    "\n",
    "One way to checking this is to plot the Independent variables in question and look for an S-shaped curve."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bc5cb30a",
   "metadata": {},
   "outputs": [],
   "source": [
    "df2           =  new_df\n",
    "target        =  'Exited'\n",
    "num_variables =  ['Tenure', 'IsActiveMember', 'Complain', 'Point Earned','Geography_code', 'Age', 'Balance', 'EstimatedSalary']\n",
    "for i in range(len(num_variables)):\n",
    "    title = num_variables[i] + '  Log odds linear plot'\n",
    "    xvar  = num_variables[i]\n",
    "    check_linearity(xvar,    df2, title, target)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "49959e51",
   "metadata": {},
   "source": [
    "#### Assumption 5 -  Logistic regression requires quite a large number of observations."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8e4e6fe0",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Number of events (cases where Response == 1)\n",
    "num_events = new_df['Exited'].sum()\n",
    "\n",
    "# Number of predictor variables (excluding 'Response')\n",
    "num_predictors = len(X_.columns)\n",
    "\n",
    "# Number of events per predictor variable\n",
    "events_per_predictor = num_events / num_predictors\n",
    "\n",
    "print(\"Number of events:\", num_events)\n",
    "print(\"Number of predictor variables:\", num_predictors)\n",
    "print(\"Events per predictor:\", events_per_predictor)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b3e7eaad",
   "metadata": {},
   "source": [
    "**Explanation:**\n",
    "\n",
    "We calculate the number of events by summing the 'Exited' column, which represents the cases where the outcome of interest occurs.\n",
    "\n",
    "We calculate the number of predictor variables by counting the number of columns in the DataFrame and excluding the outcome variable.\n",
    "\n",
    "We divide the number of events by the number of predictor variables to get the events per predictor.\n",
    "\n",
    "We can then compare the calculated events per predictor with the recommended guideline of 10-20. If the ratio is below this guideline, it may indicate a potential violation of the assumption of a sufficiently large sample size."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dacf6bed",
   "metadata": {},
   "source": [
    "### Observations:\n",
    "\n",
    "With 2038 events and 10 predictor variables, the calculated number of events per predictor is approximately 203.8. This exceeds the commonly recommended guideline of having at least 10-20 events per predictor variable.\n",
    "\n",
    "Inference: The dataset appears to meet the assumption of having a sufficiently large sample size for logistic regression.\n",
    "\n",
    "Having a high number of events per predictor variable suggests that there should be adequate statistical power and precision in estimating the model parameters, enhancing the reliability of the logistic regression analysis. Therefore, the dataset likely provides a robust basis for fitting a logistic regression model and conducting statistical inference."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a8187715",
   "metadata": {},
   "source": [
    "### Build a base model\n",
    "We are interested in the measure, recall for minority class for measuring the model performance. \n",
    "\n",
    "**Average of Recall & std. deviation in parenthesis for the minority class** of each of the  models used for comparison are listed below:\n",
    "\n",
    "#### 1) LR: 0.998037 (0.003254)\n",
    "#### 2) NB: 0.998037 (0.003254)\n",
    "#### 3) RF: 0.998037 (0.003254)\n",
    "#### 4) Decision Tree (aka CART): 0.992642 (0.005025)\n",
    "#### 5) KNN: 0.065754 (0.017023)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "45605b8d",
   "metadata": {},
   "source": [
    "Based on the above models comparison. we are choosing **Logistic Regression** as our base model.\n",
    "Logistic Regression models have high interpretability compared to most classification algorithms due to optimized feature coefficients. Refer: https://www.codecademy.com/learn/machine-learning-logistic-regression/modules/dspath-logistic-regression/cheatsheet\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "126352a0",
   "metadata": {},
   "outputs": [],
   "source": [
    "start_time = time.time()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b8bf4473",
   "metadata": {},
   "outputs": [],
   "source": [
    "## Split the data into train and test datasets\n",
    "X_train, X_test, Y_train, Y_test  = train_test_split(X_, y, stratify = y,test_size = 0.30)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "581bbd14",
   "metadata": {},
   "outputs": [],
   "source": [
    "vals, counts = np.unique(Y_test, return_counts = True)\n",
    "print(vals, counts)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e07251ea",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"\\nShape: Total observations %d Total features %d\" %(X_train.shape[0], X_train.shape[1]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6b5e02cb",
   "metadata": {},
   "outputs": [],
   "source": [
    "logit = sm.Logit( Y_train, sm.add_constant( X_train ) )\n",
    "lg    = logit.fit()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "50c04885",
   "metadata": {},
   "source": [
    "#### Report Psuedo R-square, model coefficients and p-value"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "41c6222a",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(lg.summary())"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5965fc9c",
   "metadata": {},
   "source": [
    "**Observation**\n",
    "\n",
    "We observe that the McFadden R square (Pseudo R square) is 98.20 % and the model fitness is very good.\n",
    "This McFadden approach is one minus the ratio of two log likelihoods. The numerator is the log likelihood of the logit model selected and the denominator is the log likelihood if the model just had an intercept.\n",
    "\n",
    "A goodness of fit using McFaddens pseudo r square (^2) is used for fitting the overall model. McFadden suggested ^2 values of between 0.2 and 0.4 should be taken to represent a very good fit of the model (Louviere et al.,2000).\n",
    "http://www.lifesciencesite.com/lsj/life1002/286_B01288life1002_2028_2036.pdf"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "99fc40ee",
   "metadata": {},
   "source": [
    "### List the significant variables at 5% level of significance"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3f66036b",
   "metadata": {},
   "outputs": [],
   "source": [
    "significant_vars = get_significant_vars( lg, sig_level = 0.05 )\n",
    "print(significant_vars)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "63cc718e",
   "metadata": {},
   "source": [
    "**Observation**\n",
    "\n",
    "The following variables are significant at 5 % level of significance:\n",
    "\n",
    "| SlNo | Significant variable |                      | \n",
    "| ---- | -------------------- | -------------------- |   \n",
    "| 1 | IsActiveMember | Whether the customer is active or not.| \n",
    "| 2 | Complain |Whether the customer has complaint or not. | \n",
    "| 3 | Age | Age of the customer | "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f1ab7ddb",
   "metadata": {},
   "source": [
    "### Get Odds ratio"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "af375425",
   "metadata": {},
   "outputs": [],
   "source": [
    "significant_vars"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "285be877",
   "metadata": {},
   "outputs": [],
   "source": [
    "ODDs_Ratio_df =  pd.DataFrame({'Important Variable' : lg.params.index, 'Log-odds' : lg.params.values })"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5e92c374",
   "metadata": {},
   "outputs": [],
   "source": [
    "ODDs_Ratio_df   =  ODDs_Ratio_df.loc[ODDs_Ratio_df['Important Variable'].isin(significant_vars), :]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c20802c4",
   "metadata": {},
   "outputs": [],
   "source": [
    "ODDs_Ratio_df.drop(0, inplace = True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5bc7e647",
   "metadata": {},
   "outputs": [],
   "source": [
    "ODDs_Ratio_df['Odds Ratio'] =  np.exp(ODDs_Ratio_df['Log-odds'])\n",
    "ODDs_Ratio_df.sort_values(by=['Odds Ratio'], ascending=False, inplace = True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5b7ccc1e",
   "metadata": {},
   "outputs": [],
   "source": [
    "ODDs_Ratio_df"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3f5ecd58",
   "metadata": {},
   "source": [
    "### Odds Ratio Interpretation for significant variables"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e5feedea",
   "metadata": {},
   "source": [
    "Holding other things constant:\n",
    "\n",
    "| Slno | Inference |\n",
    "| ----- | ------------------------------------- |\n",
    "| 1 | The odds of customer churn are 567627 times the odds of customer retention when the customer has complaints.|\n",
    "| 2 | The odds of customer churn are approximately 2.78 times the odds of customer retention for one unit increase in Age.|"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d5a72fbe",
   "metadata": {},
   "outputs": [],
   "source": [
    "n_splits, random_state = 10, 12345\n",
    "                     \n",
    "print(logit_reg(n_splits, random_state, X_train, X_test, Y_train, Y_test ))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ba32e111",
   "metadata": {},
   "source": [
    "### To get the elapsed time, get the end time of processing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "40533044",
   "metadata": {},
   "outputs": [],
   "source": [
    "logit_elapsed_time_secs = time.time() - start_time\n",
    "\n",
    "logit_elapsed_time_msg = \"LR model - Execution took: %s secs (Wall clock time)\" % timedelta(seconds=round(logit_elapsed_time_secs))\n",
    "\n",
    "print(logit_elapsed_time_msg)   "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f1248adf",
   "metadata": {},
   "source": [
    "## END"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
